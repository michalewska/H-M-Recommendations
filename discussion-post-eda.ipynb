{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Libraries\nimport os\nimport gc\nimport wandb\nimport time\nimport random\nimport math\nimport glob\nfrom scipy import spatial\nfrom tqdm import tqdm\nimport warnings\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom numpy import dot, sqrt\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\nfrom IPython.display import display_html\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nplt.rcParams.update({'font.size': 16})\n\n# HELPER FUNCTIONS\ndef adjust_id(x):\n    '''Adjusts article ID code.'''\n    x = str(x)\n    if len(x) == 9:\n        x = \"0\"+x\n    \n    return x\n\n\ndef insert_image(path, zoom, xybox, ax):\n    '''Insert an image within matplotlib'''\n    imagebox = OffsetImage(mpimg.imread(path), zoom=zoom)\n    ab = AnnotationBbox(imagebox, xy=(0.5, 0.7), frameon=False, pad=1, xybox=xybox)\n    ax.add_artist(ab)\n    \n    \ndef show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    '''Plots the value at the end of the a seaborn barplot.\n    axs: the ax of the plot\n    h_v: weather or not the barplot is vertical/ horizontal'''\n    \n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, format(value, ','), ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, format(value, ','), ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\n\n# === üêù W&B ===\ndef save_dataset_artifact(run_name, artifact_name, path):\n    '''Saves dataset to W&B Artifactory.\n    run_name: name of the experiment\n    artifact_name: under what name should the dataset be stored\n    path: path to the dataset'''\n    \n    run = wandb.init(project='HandM', \n                     name=run_name, \n                     config=CONFIG)\n    artifact = wandb.Artifact(name=artifact_name, \n                              type='dataset')\n    artifact.add_file(path)\n\n    wandb.log_artifact(artifact)\n    wandb.finish()\n    print(\"Artifact has been saved successfully.\")\n    \n    \ndef create_wandb_plot(x_data=None, y_data=None, x_name=None, y_name=None, title=None, log=None, plot=\"line\"):\n    '''Create and save lineplot/barplot in W&B Environment.\n    x_data & y_data: Pandas Series containing x & y data\n    x_name & y_name: strings containing axis names\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[label, val] for (label, val) in zip(x_data, y_data)]\n    table = wandb.Table(data=data, columns = [x_name, y_name])\n    \n    if plot == \"line\":\n        wandb.log({log : wandb.plot.line(table, x_name, y_name, title=title)})\n    elif plot == \"bar\":\n        wandb.log({log : wandb.plot.bar(table, x_name, y_name, title=title)})\n    elif plot == \"scatter\":\n        wandb.log({log : wandb.plot.scatter(table, x_name, y_name, title=title)})\n        \n        \ndef create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n    '''Create and save histogram in W&B Environment.\n    x_data: Pandas Series containing x values\n    x_name: strings containing axis name\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[x] for x in x_data]\n    table = wandb.Table(data=data, columns=[x_name])\n    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})\n    \n\n# Environment check\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"WANDB_SILENT\"] = \"true\"\nCONFIG = {'competition': 'HandM', '_wandb_kernel': 'aot'}\n\n# Custom colors\nclass clr:\n    S = '\\033[1m' + '\\033[95m'\n    E = '\\033[0m'\n    \nmy_colors = [\"#AF0848\", \"#E90B60\", \"#CB2170\", \"#954E93\", \"#705D98\", \"#5573A8\", \"#398BBB\", \"#00BDE3\"]\nprint(clr.S+\"Notebook Color Scheme:\"+clr.E)\nsns.palplot(sns.color_palette(my_colors))\nplt.show()\n\nbk_image = plt.imread(\"../input/hm-fashion-recommender-dataset/background.jpg\")\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n! wandb login 774afba7e51bd512db78d9b36590060e3739ca8c\n\n# Read in the data\narticles = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\")\ncustomers = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\")\ntransactions = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")\nss = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv\")\n\n# ARTICLES\nprint(clr.S+\"ARTICLES:\"+clr.E, articles.shape)\ndisplay_html(articles.head(3))\nprint(\"\\n\", clr.S+\"CUSTOMERS:\"+clr.E, customers.shape)\ndisplay_html(customers.head(3))\nprint(\"\\n\", clr.S+\"TRANSACTIONS:\"+clr.E, transactions.shape)\ndisplay_html(transactions.head(3))\nprint(\"\\n\", clr.S+\"SAMPLE_SUBMISSION:\"+clr.E, ss.shape)\ndisplay_html(ss.head(3))\n\n# üêù W&B Experiment\nrun = wandb.init(project='HandM', name='Articles', config=CONFIG)\n\nprint(clr.S+\"There are no missing values in any columns but 'Detail Description':\"+clr.E,\n      articles.isna().sum()[-1], \"total missing values\")\n\n# Replace missing values\narticles.fillna(value=\"No Description\", inplace=True)\n\n# Adjust the article ID and product code to be string & add \"0\"\narticles[\"article_id\"] = articles[\"article_id\"].apply(lambda x: adjust_id(x))\narticles[\"product_code\"] = articles[\"article_id\"].apply(lambda x: x[:3])\n\nall_image_paths = glob.glob(f\"../input/h-and-m-personalized-fashion-recommendations/images/*/*\")\n\nprint(clr.S+\"Number of unique article_ids within articles.csv:\"+clr.E, len(articles), \"\\n\"+\n      clr.S+\"Number of unique images within the image folder:\"+clr.E, len(all_image_paths), \"\\n\"+\n      clr.S+\"=> not all article_ids have a corresponding image!!!\"+clr.E, \"\\n\")\n\n# üêù Log Distinct article IDs\nwandb.log({\"article_ids\":len(articles)})\n\n# Get all valid article ids\n# Create a set() - as it moves faster than a list\nall_image_ids = set()\n\nfor path in tqdm(all_image_paths):\n    article_id = path.split('/')[-1].split('.')[0]\n    all_image_ids.add(article_id)\n    \n# Create full path to the article image\nimages_path = \"../input/h-and-m-personalized-fashion-recommendations/images/\"\narticles[\"path\"] = images_path + articles[\"product_code\"] + \"/\" + articles[\"article_id\"] + \".jpg\"\n\n# Adjust the incorrect paths and set them to None\nfor k, article_id in tqdm(enumerate(articles[\"article_id\"])):\n    if article_id not in all_image_ids:\n        articles.loc[k, \"path\"] = None\n        \nprint(clr.S+\"Total Number of unique Product Names:\"+clr.E, articles[\"prod_name\"].nunique())\n\n# Data\nprod_name = articles[\"prod_name\"].value_counts().reset_index().head(15)\ntotal_prod_names = articles[\"prod_name\"].nunique()\nclrs = [\"#CB2170\" if x==max(prod_name[\"prod_name\"]) else '#954E93' for x in prod_name[\"prod_name\"]]\n\n# Get images\nprod_name_images = articles[articles[\"prod_name\"].isin(prod_name[\"index\"].tolist())].groupby(\"prod_name\")[\"path\"].first().reset_index()\nimage_paths = prod_name_images[\"path\"].tolist()\nimage_names = prod_name_images[\"prod_name\"].tolist()\n\n# Plot\nfig, ax = plt.subplots(figsize=(25, 13))\nplt.title('- Most Frequent Product Names -', size=22, weight=\"bold\")\n\nsns.barplot(data=prod_name, x=\"prod_name\", y=\"index\", ax=ax,\n            palette=clrs)\nx0,x1 = ax.get_xlim()\ny0,y1 = ax.get_ylim()\nplt.imshow(bk_image, zorder=0, extent=[x0, x1, y0, y1], alpha=0.35, aspect='auto')\n\nshow_values_on_bars(axs=ax, h_v=\"h\", space=0.4)\nplt.ylabel(\"Product Name\", size = 16, weight=\"bold\")\nplt.xlabel(\"\")\nplt.xticks([])\nplt.yticks(size=16)\nplt.tick_params(size=16)\n\ninsert_image(path='../input/hm-fashion-recommender-dataset/pics/dragonfly.jpg', zoom=0.45, xybox=(92, 11), ax=ax)\n\nsns.despine(left=True, bottom=True)\nplt.show();\n\nprint(\"\\n\")\n\n# Plot\nfig, axs = plt.subplots(3, 5, figsize=(23, 8))\nfig.suptitle('- Example Images -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths, image_names)):\n    axs[k].set_title(f\"{name}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n        \n# üêù Log Barplot to W&B\ncreate_wandb_plot(x_data=prod_name[\"index\"], y_data=prod_name[\"prod_name\"],\n                  x_name=\"Product Name\", y_name=\"Frequency\", \n                  title=\"- Most Frequent Product Names -\", log=\"prod_name\", plot=\"bar\")\n\nprint(clr.S+\"Total Number of unique Product Types:\"+clr.E, articles[\"product_type_name\"].nunique())\n\n# Data\nprod_type = articles[\"product_type_name\"].value_counts().reset_index().head(15)\ntotal_prod_types = articles[\"product_type_name\"].nunique()\nclrs = [\"#00BDE3\" if x==max(prod_type[\"product_type_name\"]) else '#398BBB' for x in prod_type[\"product_type_name\"]]\n\n# Get images\nprod_type_images = articles[articles[\"product_type_name\"].isin(prod_type[\"index\"].tolist())].groupby(\"product_type_name\")[\"path\"].first().reset_index()\nimage_paths = prod_type_images[\"path\"].tolist()\nimage_names = prod_type_images[\"product_type_name\"].tolist()\n\n# Plot\nfig, ax = plt.subplots(figsize=(25, 13))\nplt.title('- Most Frequent Product Types -', size=22, weight=\"bold\")\n\nsns.barplot(data=prod_type, x=\"product_type_name\", y=\"index\", ax=ax,\n            palette=clrs)\nx0,x1 = ax.get_xlim()\ny0,y1 = ax.get_ylim()\nplt.imshow(bk_image, zorder=0, extent=[x0, x1, y0, y1], alpha=0.35, aspect='auto')\n\nshow_values_on_bars(axs=ax, h_v=\"h\", space=0.4)\nplt.ylabel(\"Product Type\", size = 16, weight=\"bold\")\nplt.xlabel(\"\")\nplt.xticks([])\nplt.yticks(size=16)\nplt.tick_params(size=16)\n\ninsert_image(path='../input/hm-fashion-recommender-dataset/pics/blue.jpg', zoom=0.45, xybox=(11000, 11), ax=ax)\n\nsns.despine(left=True, bottom=True)\nplt.show();\n\nprint(\"\\n\")\n\n# Plot\nfig, axs = plt.subplots(3, 5, figsize=(23, 8))\nfig.suptitle('- Example Images -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths, image_names)):\n    axs[k].set_title(f\"{name}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n# üêù Log Barplot to W&B\ncreate_wandb_plot(x_data=prod_type[\"index\"], y_data=prod_type[\"product_type_name\"],\n                  x_name=\"Product Type\", y_name=\"Frequency\", \n                  title=\"- Most Frequent Product Types -\", log=\"prod_type\", plot=\"bar\")\n\nprint(clr.S+\"Total Number of unique Product Group:\"+clr.E, articles[\"product_group_name\"].nunique())\n\n# Data\nprod_group = articles[\"product_group_name\"].value_counts().reset_index()\ntotal_prod_groups = articles[\"product_group_name\"].nunique()\nclrs = [\"#E90B60\" if x==max(prod_group[\"product_group_name\"]) else '#AF0848' for x in prod_group[\"product_group_name\"]]\n\n# Get images\nprod_group_images = articles[articles[\"product_group_name\"].isin(prod_group[\"index\"].tolist())].groupby(\"product_group_name\")[\"path\"].first().reset_index()\nimage_paths = prod_group_images[\"path\"].tolist()\nimage_names = prod_group_images[\"product_group_name\"].tolist()\n\n# Plot\nfig, ax = plt.subplots(figsize=(25, 13))\nplt.title('- Most Frequent Product Groups -', size=22, weight=\"bold\")\n\nsns.barplot(data=prod_group, x=\"product_group_name\", y=\"index\", ax=ax,\n            palette=clrs)\nx0,x1 = ax.get_xlim()\ny0,y1 = ax.get_ylim()\nplt.imshow(bk_image, zorder=0, extent=[x0, x1, y0, y1], alpha=0.35, aspect='auto')\n\nshow_values_on_bars(axs=ax, h_v=\"h\", space=0.4)\nplt.ylabel(\"Product Group\", size = 16, weight=\"bold\")\nplt.xlabel(\"\")\nplt.xticks([])\nplt.yticks(size=16)\nplt.tick_params(size=16)\n\ninsert_image(path='../input/hm-fashion-recommender-dataset/pics/chloe.jpg', zoom=0.45, xybox=(40000, 14), ax=ax)\n\nsns.despine(left=True, bottom=True)\nplt.show();\n\nprint(\"\\n\")\n\n# Plot\nfig, axs = plt.subplots(4, 6, figsize=(23, 10))\nfig.suptitle('- Example Images -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths, image_names)):\n    axs[k].set_title(f\"{name}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\nfor a in [-1, -2, -3, -4, -5]: axs[a].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n# üêù Log Barplot to W&B\ncreate_wandb_plot(x_data=prod_group[\"index\"], y_data=prod_group[\"product_group_name\"],\n                  x_name=\"Product Group\", y_name=\"Frequency\", \n                  title=\"- Most Frequent Product Group -\", log=\"prod_group\", plot=\"bar\")\n\ndef change_color(x):\n    '''Change color name.'''\n    if (\"light\" in x.lower().strip()) or \\\n        (\"dark\" in x.lower().strip()) or \\\n        (\"greyish\" in x.lower().strip()) or \\\n        (\"yellowish\" in x.lower().strip()) or \\\n        (\"greenish\" in x.lower().strip()) or \\\n        (\"off\" in x.lower().strip()) or \\\n        (\"other\" in x.lower().strip()):\n        x = x.split(\" \")[-1]\n        \n    return x\n\narticles[\"colour_group_name\"] = articles[\"colour_group_name\"].apply(lambda x: change_color(x))\n\n# Appearance and color\nprint(clr.S+\"Total Number of unique Product Appearances:\"+clr.E, articles[\"graphical_appearance_name\"].nunique())\nprint(clr.S+\"Total Number of unique Product Colors (after preprocess):\"+clr.E, articles[\"colour_group_name\"].nunique())\n\n# --- Data 1 ---\nprod_appearance = articles[\"graphical_appearance_name\"].value_counts().reset_index().head(15)\ntotal_prod_appearances = articles[\"graphical_appearance_name\"].nunique()\nclrs1 = [\"#AF0848\" if x==max(prod_appearance[\"graphical_appearance_name\"]) else '#E90B60' for x in prod_appearance[\"graphical_appearance_name\"]]\n\n\n# Get images\nprod_appearance_images = articles[articles[\"graphical_appearance_name\"].isin(prod_appearance[\"index\"].tolist())].groupby(\"graphical_appearance_name\")[\"path\"].first().reset_index()\nimage_paths1 = prod_appearance_images[\"path\"].tolist()\nimage_names1 = prod_appearance_images[\"graphical_appearance_name\"].tolist()\n\n# --- Data 2 ---\nprod_color = articles[\"colour_group_name\"].value_counts().reset_index().head(15)\ntotal_prod_color = articles[\"colour_group_name\"].nunique()\nclrs2 = [\"#CB2170\" if x==max(prod_color[\"colour_group_name\"]) else '#954E93' for x in prod_color[\"colour_group_name\"]]\n\n# Get images\nprod_color_images = articles[articles[\"colour_group_name\"].isin(prod_color[\"index\"].tolist())].groupby(\"colour_group_name\")[\"path\"].first().reset_index()\nimage_paths2 = prod_color_images[\"path\"].tolist()\nimage_names2 = prod_color_images[\"colour_group_name\"].tolist()\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25, 13))\n\nax1.set_title('- Most Frequent Product Appearances -', size=22, weight=\"bold\")\nsns.barplot(data=prod_appearance, x=\"graphical_appearance_name\", y=\"index\", ax=ax1,\n            palette=clrs2)\nx0,x1 = ax1.get_xlim()\ny0,y1 = ax1.get_ylim()\nax1.imshow(bk_image, zorder=0, extent=[x0, x1, y0, y1], alpha=0.35, aspect='auto')\n\nshow_values_on_bars(axs=ax1, h_v=\"h\", space=0.4)\nax1.set_ylabel(\"Product Appearance\", size = 16, weight=\"bold\")\nax1.set_xlabel(\"\")\nax1.set_xticks([])\n# ax1.set_yticks(size=16)\n# ax1.set_tick_params(size=16)\n\n# insert_image(path='../input/hm-fashion-recommender-dataset/pics/blue.jpg', zoom=0.45, xybox=(11000, 11), ax=ax1)\n\n\nax2.set_title('- Most Frequent Product Colors -', size=22, weight=\"bold\")\nsns.barplot(data=prod_color, x=\"colour_group_name\", y=\"index\", ax=ax2,\n            palette=clrs2)\nx0,x1 = ax2.get_xlim()\ny0,y1 = ax2.get_ylim()\nax2.imshow(bk_image, zorder=0, extent=[x0, x1, y0, y1], alpha=0.35, aspect='auto')\n\nshow_values_on_bars(axs=ax2, h_v=\"h\", space=0.4)\nax2.set_ylabel(\"Product Colors\", size = 16, weight=\"bold\")\nax2.set_xlabel(\"\")\nax2.set_xticks([])\n# ax1.set_yticks(size=16)\n# ax1.set_tick_params(size=16)\n\n# insert_image(path='../input/hm-fashion-recommender-dataset/pics/blue.jpg', zoom=0.45, xybox=(11000, 11), ax=ax1)\n\nsns.despine(left=True, bottom=True)\nplt.show();\n\nprint(\"\\n\")\n\n# Plot\nfig, axs = plt.subplots(3, 5, figsize=(23, 8))\nfig.suptitle('- Example Images [Appearance] -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths1, image_names1)):\n    axs[k].set_title(f\"{name}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n# Plot\nfig, axs = plt.subplots(3, 5, figsize=(23, 8))\nfig.suptitle('- Example Images [Color] -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths2, image_names2)):\n    axs[k].set_title(f\"{name}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n# üêù Log Barplot to W&B\ncreate_wandb_plot(x_data= prod_appearance[\"index\"], y_data=prod_appearance[\"graphical_appearance_name\"],\n                  x_name=\"Product Appearance\", y_name=\"Frequency\", \n                  title=\"- Most Frequent Product Appearance -\", log=\"prod_appearance\", plot=\"bar\")\n\ncreate_wandb_plot(x_data= prod_color[\"index\"], y_data=prod_color[\"colour_group_name\"],\n                  x_name=\"Product Color\", y_name=\"Frequency\", \n                  title=\"- Most Frequent Product Color -\", log=\"prod_color\", plot=\"bar\")\n\ndef similar_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    '''Creates a custom function for the color of the wordcloud.'''\n    \n    h = 270 # 0 - 360 <- the color hue\n    s = 40 # 0-100 <- the color saturation\n    l = random_state.randint(30, 70) # 0 - 100 <- gradient\n    \n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nprint(clr.S+\"Total Number of unique Article Descriptions:\"+clr.E, articles[\"detail_desc\"].nunique(), \"\\n\")\n\n# Get descriptions and convert them to a string\ntext = articles[\"detail_desc\"].unique()\ntext = \" \".join(text)\n\n# Get the mask - the form of the wordcloud\nmask = np.array(Image.open('../input/hm-fashion-recommender-dataset/pics/mask.jpg'))\n\n# Create wordcloud object\nwc = WordCloud(mask=mask, background_color=\"white\", max_words=2000,\n               stopwords=STOPWORDS, max_font_size=256,\n               random_state=42, width=mask.shape[1],\n               height=mask.shape[0], font_path=\"../input/hm-fashion-recommender-dataset/MorningRainbow.ttf\",\n               color_func=similar_color_func)\nwc.generate(text)\n\n# Plot\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words found within Article Descriptions -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n# üêù Save wordcloud to Dashboard\nfig.canvas.draw()\nimage_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\nimage_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\nwandb.log({\"wordcloud\": wandb.Image(image_from_plot)})\n\nwandb.finish()\n\n# üêù Save the updated `articles` file.\n# articles.to_parquet('articles.pqt', index=False)\n\nsave_dataset_artifact(run_name=\"save_articles\", artifact_name=\"articles\",\n                      path=\"../input/hm-fashion-recommender-dataset/articles.pqt\")\n        \n# CUSTOMERS \nrun = wandb.init(project='HandM', name='Customers', config=CONFIG)\n\ndef create_age_interval(x):\n    if x <= 25:\n        return [16, 25]\n    elif x <= 35:\n        return [26, 35]\n    elif x <= 45:\n        return [36, 45]\n    elif x <= 55:\n        return [46, 55]\n    elif x <= 65:\n        return [56, 65]\n    else:\n        return [66, 99]\n    \nprint(clr.S+\"Missing values within customers dataset:\"+clr.E)\nprint(customers.isna().sum())\n\n# üêù Log Distinct customer IDs\nwandb.log({\"customer_ids\":len(customers)})\n\n# Fill FN and Active - the only available value is \"1\"\ncustomers[\"FN\"].fillna(0, inplace=True)\ncustomers[\"Active\"].fillna(0, inplace=True)\n\n# Set unknown the club member status & news frequency\ncustomers[\"club_member_status\"].fillna(\"UNKNOWN\", inplace=True)\n\ncustomers[\"fashion_news_frequency\"] = customers[\"fashion_news_frequency\"].replace({\"None\":\"NONE\"})\ncustomers[\"fashion_news_frequency\"].fillna(\"UNKNOWN\", inplace=True)\n\n# Set missing values in age with the median\ncustomers[\"age\"].fillna(customers[\"age\"].median(), inplace=True)\ncustomers[\"age_interval\"] = customers[\"age\"].apply(lambda x: create_age_interval(x))\n\nsave_dataset_artifact(run_name=\"save_customers\", artifact_name=\"customers\",\n                      path=\"../input/hm-fashion-recommender-dataset/customers.pqt\")\n\n# TRANSACTIONS\n# üêù W&B Experiment\n\nrun = wandb.init(project='HandM', name='Transactions', config=CONFIG)\n\nprint(clr.S+\"Missing values within transactions dataset:\"+clr.E)\nprint(transactions.isna().sum())\n\n# üêù Log length of transactions\nwandb.log({\"transaction_ids\":len(transactions)})\n\n# Adjust article_id (as did for articles dataframe)\ntransactions[\"article_id\"] = transactions[\"article_id\"].apply(lambda x: adjust_id(x))\n\nprint(clr.S+\"Maximum Price is:\"+clr.E, transactions[\"price\"].max(), \"\\n\" +\n      clr.S+\"Minimum Price is:\"+clr.E, transactions[\"price\"].min(), \"\\n\" +\n      clr.S+\"Average Price is:\"+clr.E, transactions[\"price\"].mean())\n\n# Get data\ntop_sold_products = transactions.groupby(\"article_id\")[\"price\"].max().reset_index()\\\n                                        .sort_values(\"price\", ascending=False).head(15)\ntop_sold_products.columns = [\"article_id\", \"price\"]\ntop_sold_products = pd.merge(top_sold_products, articles, on=\"article_id\")[[\"article_id\", \"price\", \"prod_name\"]]\n\nclrs = [\"#E90B60\" if x==max(top_sold_products[\"price\"]) else '#AF0848' for x in top_sold_products[\"price\"]]\n\n# Get images\nimage_paths = [path for path in articles[articles[\"article_id\"].isin(top_sold_products[\"article_id\"].tolist())][\"path\"].tolist() \n               if path != None]\nimage_names = articles[articles[\"path\"].isin(image_paths)][\"prod_name\"].tolist()\n\n# Plot\nfig, axs = plt.subplots(3, 5, figsize=(23, 10))\nfig.suptitle('- Most Expensive Products -', size=22, weight=\"bold\")\naxs = axs.flatten()\n\nfor k, (path, name) in enumerate(zip(image_paths, image_names)):\n    prc = top_sold_products[top_sold_products[\"prod_name\"]==name][\"price\"].values[0]\n    axs[k].set_title(f\"{name} : {round(prc, 3)}\", size = 16)\n    img = plt.imread(path)\n    axs[k].imshow(img)\n    axs[k].axis(\"off\")\n\n# for a in [-1, -2]: axs[a].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n# Data\nbasket = transactions.groupby(\"customer_id\").agg({'article_id':'count', \n                                                  'price': 'sum'}).reset_index()\nbasket.columns = [\"customer_id\", \"units\", \"order_price\"]\n\nprint(clr.S+\"=== UNITS/ORDER ===\"+clr.E)\nprint(clr.S+\"Maximum Units/Order is:\"+clr.E, basket[\"units\"].max(), \"\\n\" +\n      clr.S+\"Minimum Units/Order is:\"+clr.E, basket[\"units\"].min(), \"\\n\" +\n      clr.S+\"Average Units/Order is:\"+clr.E, basket[\"units\"].mean(), \"\\n\")\n\nprint(clr.S+\"=== SPENDING/ORDER ===\"+clr.E)\nprint(clr.S+\"Maximum Spending/Order is:\"+clr.E, basket[\"order_price\"].max(), \"\\n\" +\n      clr.S+\"Minimum Spending/Order is:\"+clr.E, basket[\"order_price\"].min(), \"\\n\" +\n      clr.S+\"Average Spending/Order is:\"+clr.E, basket[\"order_price\"].mean())\n\n# Plot\nplt.figure(figsize=(24, 15))\nplt.suptitle('- Order Attributes -', size=22, weight=\"bold\")\n\nax1 = plt.subplot(2,2,1)\nax2 = plt.subplot(2,2,2)\nax3 = plt.subplot(2,1,2)\n\nsns.distplot(basket[\"units\"], color=my_colors[-3], ax=ax1,\n             hist_kws=dict(edgecolor=my_colors[-3]))\nax1.set_title(\"Units/Order Distribution\", size=18, weight=\"bold\")\nax1.set_ylabel(\"\")\n\nsns.distplot(basket[\"order_price\"], color=my_colors[-5], ax=ax2,\n             hist_kws=dict(edgecolor=my_colors[-5]))\nax2.set_title(\"Spending/Order Distribution\", size=18, weight=\"bold\")\nax2.set_ylabel(\"\")\n\nsns.scatterplot(data=basket, x=\"units\", y=\"order_price\", hue=\"units\", palette=\"mako\", \n                legend=None, ax=ax3)\nax3.set_title(\"Units x Price Correlation\", size=18, weight=\"bold\")\nax3.set_ylabel(\"Spending per Order\")\nax3.set_xlabel(\"Units per Order\")\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None);\n\nsave_dataset_artifact(run_name=\"save_transactions\", artifact_name=\"transactions\",\n                      path=\"../input/hm-fashion-recommender-dataset/transactions.pqt\")\n\n# MARKET BASKET ANALYSIS\n!pip install turicreate --user\nimport turicreate as tc\nimport cudf\nimport cuml\nimport cupy\n\nfrom cuml.model_selection import train_test_split\n\ntransactions = cudf.read_parquet(\"../input/hm-fashion-recommender-dataset/transactions.pqt\")\n# Keep only last 16 digits from customer_id and convert to int\ntransactions['customer_id'] = transactions['customer_id'].str[-16:].str.hex_to_int().astype('int64')\n# Convert article_id from object to int32\n# transactions['article_id'] = transactions['article_id'].astype('int32')\n# Convert date from object to datetime\ntransactions['t_dat'] = cudf.to_datetime(transactions['t_dat'])\n\ntransactions = transactions[['t_dat','customer_id','article_id']]\n\n# ------ PARAMETERS ------\nTOP_CUSTOMERS = 300000\nTOP_N = 200\n# ------------------------\n\n# Select only most frequent article ids\nmost_frequent_articles = transactions[\"article_id\"].value_counts().reset_index()\nmost_frequent_articles.columns = [\"article_id\", \"count\"]\nprint(clr.S+\"Total Unique IDs in Transactions:\"+clr.E, len(most_frequent_articles))\nprint(clr.S+\"Total Unique IDs that are selected:\"+clr.E, TOP_N)\n# Get top n most frequent products\nmost_frequent_articles = cupy.asarray(most_frequent_articles.head(TOP_N)[\"article_id\"])\n\ntransactions = transactions[transactions[\"article_id\"].isin(most_frequent_articles)].reset_index(drop=True)\n\n# Get customers with many transactions on these TOP_N items\ncustomers_top_trans = list(transactions[\"customer_id\"].value_counts().reset_index()\\\n                            .head(TOP_CUSTOMERS)[\"index\"].unique().to_pandas())\ntransactions = transactions[transactions[\"customer_id\"].isin(customers_top_trans)].reset_index(drop=True)\n\nprint(clr.S+\"Total unique users to recommend:\"+clr.E, transactions[\"customer_id\"].nunique())\n\ndel most_frequent_articles, customers_top_trans\ngc.collect()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-12T21:16:01.368347Z","iopub.execute_input":"2022-04-12T21:16:01.368623Z","iopub.status.idle":"2022-04-12T21:22:18.798242Z","shell.execute_reply.started":"2022-04-12T21:16:01.368591Z","shell.execute_reply":"2022-04-12T21:22:18.797492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.21.06 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:31:47.5808Z","iopub.execute_input":"2022-04-12T18:31:47.582086Z","iopub.status.idle":"2022-04-12T18:34:14.311964Z","shell.execute_reply.started":"2022-04-12T18:31:47.582035Z","shell.execute_reply":"2022-04-12T18:34:14.310992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN DATASET\n\n# Count per each customer how many products of each they have bought\ntrain = transactions.groupby([\"customer_id\",\"article_id\"])[\"t_dat\"].count().reset_index()\ntrain.columns = [\"customer_id\",\"article_id\", \"purchase_count\"]\n\ntrain.head()\n\n# DUMMY DATASET \ndummy_train = train.copy()\ndummy_train['purchase_dummy'] = 1\n\ndummy_train.head()\n\n# NORMALIZED DATASET \ndef normalize_data(data):\n    \n    # Create matrix with customers on rows and articles ad columns\n    # Using pandas here due to an error in cudf\n    df_matrix = cudf.DataFrame(pd.pivot(data.to_pandas(), columns=\"article_id\",\n                                        index=\"customer_id\", values=\"purchase_count\"))\n    # Normalize\n    df_matrix_norm = (df_matrix-df_matrix.min())/(df_matrix.max()-df_matrix.min())\n    d = df_matrix_norm.reset_index()\n    d.index.names = ['scaled_purchase_freq']\n    \n    # Recreate df with each customer and the 'label' whether they purchased the product or not\n    final = cudf.melt(d, id_vars=['customer_id'], value_name='scaled_purchase_freq').dropna()\n    final.columns = [\"customer_id\", \"article_id\", \"scaled_purchase_freq\"]\n    final = final.reset_index(drop=True)\n    \n    return final\n\n# Normalize count dataset\nnorm_train = normalize_data(data=train)\n\nnorm_train.head()\n\ndef split_data(data):\n    \n    train, test = train_test_split(data, test_size=0.3)\n    train_data = tc.SFrame(train.to_pandas())\n    test_data = tc.SFrame(test.to_pandas())\n    \n    return train_data, test_data\n\n\n# TODO: fix norm_train error\ntrain_data, test_data = split_data(train)\ntrain_data_dummy, test_data_dummy = split_data(dummy_train)\n# train_data_norm, test_data_norm = split_data(norm_train)\n\n# ------ PARAMETERS ------\nuser_id = 'customer_id'\nitem_id = 'article_id'\nusers_to_recommend = list(train[\"customer_id\"].unique().to_pandas())\n# ------------------------\n\ndef train_model(train_data, name, user_id, item_id, target, users_to_recommend):\n    '''\n    Trains a recommender model.\n    train_data: the training tc.SFrame()\n    name: can be 'popularity', 'cosine' or 'pearson'\n    user_id & item_id: the customer and article unique IDs\n    target: the value to be predicted, can be 'purchase_count', 'purchase_dummy' or 'scaled_purchase_freq'\n    users_to_recommend: a unique list containing all customers for which we do the prediction\n    '''\n    \n    if name == 'popularity':\n        model = tc.popularity_recommender.create(train_data, \n                                                 user_id=user_id, \n                                                 item_id=item_id, \n                                                 target=target, verbose=False)\n    elif name == 'cosine':\n        model = tc.item_similarity_recommender.create(train_data, \n                                                      user_id=user_id, \n                                                      item_id=item_id, \n                                                      target=target,\n                                                      similarity_type='cosine', verbose=False)\n    elif name == 'pearson':\n            model = tc.item_similarity_recommender.create(train_data, \n                                                          user_id=user_id, \n                                                          item_id=item_id, \n                                                          target=target, \n                                                          similarity_type='pearson', verbose=False)\n    \n    # k is set to 12 => maximum items to recommend for one customer\n    recom = model.recommend(users=users_to_recommend, k=12, verbose=False)\n    \n    return model, recom\n\n# --- TRAIN ---\nname = 'popularity'\ntarget = 'purchase_count'\n\npopularity_count, _ = train_model(train_data, name, user_id, item_id, target, users_to_recommend)\n# _.to_dataframe()[\"article_id\"].nunique()\n\n\n# --- DUMMY ---\nname = 'popularity'\ntarget = 'purchase_dummy'\n\npopularity_dummy, _ = train_model(train_data_dummy, name, user_id, item_id, target, users_to_recommend)\n\n\n# --- TRAIN ---\nname = 'cosine'\ntarget = 'purchase_count'\n\ncosine_count, _ = train_model(train_data, name, user_id, item_id, target, users_to_recommend)\n\n\n# --- DUMMY ---\nname = 'cosine'\ntarget = 'purchase_dummy'\n\ncosine_dummy, _ = train_model(train_data_dummy, name, user_id, item_id, target, users_to_recommend)\n\n\n# --- TRAIN ---\nname = 'pearson'\ntarget = 'purchase_count'\n\npearson_count, _ = train_model(train_data, name, user_id, item_id, target, users_to_recommend)\n\n\n# --- DUMMY ---\nname = 'pearson'\ntarget = 'purchase_dummy'\n\npearson_dummy, _ = train_model(train_data_dummy, name, user_id, item_id, target, users_to_recommend)\n\n\n# Group all models per type of dataset\nmodel_counts = [popularity_count, cosine_count, pearson_count]\nmodels_dummy = [popularity_dummy, cosine_dummy, pearson_dummy]\n\nnames_counts = ['Popularity Model on Purchase Counts', 'Cosine Similarity on Purchase Counts',\n                'Pearson Similarity on Purchase Counts']\nnames_dummy = ['Popularity Model on Purchase Dummy', 'Cosine Similarity on Purchase Dummy',\n               'Pearson Similarity on Purchase Dummy']\n\n# # Evaluate the models\n# eval_counts = tc.recommender.util.compare_models(test_data, model_counts, model_names=names_counts, verbose=False)\n# eval_dummy = tc.recommender.util.compare_models(test_data_dummy, models_dummy, model_names=names_dummy, verbose=False)\n\n# üêù Save the updated `articles` file.\nsave_dataset_artifact(run_name=\"save_output_counts\", artifact_name=\"counts_results\",\n                      path=\"../input/hm-fashion-recommender-dataset/eval_counts.txt\")\n\nsave_dataset_artifact(run_name=\"save_output_dummy\", artifact_name=\"dummy_results\",\n                      path=\"../input/hm-fashion-recommender-dataset/eval_dummy.txt\")\n\n\ndel popularity_count, cosine_count, pearson_count\ndel popularity_dummy, cosine_dummy, pearson_dummy\ngc.collect()\n\n\nMETHOD = \"cosine\"\nTARGET = \"purchase_count\"\n\n# üêù W&B Experiment\nrun = wandb.init(project='HandM', name=f'{METHOD}_customers{TOP_CUSTOMERS}_topArticles{TOP_N}', config=CONFIG)\n\n\n# Final Model Training\nfinal_model = tc.item_similarity_recommender.create(tc.SFrame(train.to_pandas()), \n                                                    user_id=user_id, \n                                                    item_id=item_id, \n                                                    target=TARGET,\n                                                    similarity_type=METHOD,\n                                                    verbose=False)\n\nrecom = final_model.recommend(users=users_to_recommend, k=12, verbose=False)\n\n# Convert to dataframe\nrecom_df = recom.to_dataframe()\n\nprint(clr.S+\"12 Recommendations for each Customer:\"+clr.E)\nrecom_df.head(12)\n\n\n# Create predictions out of the dataset\nrecom_df = create_predictions_format(recom_df)\n\n\n# üêù Log CV score\nwandb.log({\"TOP_CUSTOMERS\" : TOP_CUSTOMERS,\n              \"TOP_N\" : TOP_N,\n              \"METHOD\" : METHOD,\n              \"TARGET\": TARGET,\n              \"CV\" : 0.0051})\n\nwandb.finish()\n\n\n# Import sample submission\nss = cudf.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv\")\nss['customer_id_new'] = ss['customer_id'].str[-16:].str.hex_to_int().astype('int64')\n\n\n# Merge with predicted preds\nss = ss.merge(cudf.DataFrame(recom_df[[\"customer_id\", \"preds\"]]), \n              left_on=\"customer_id_new\", right_on=\"customer_id\", how=\"left\")\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:42.084703Z","iopub.execute_input":"2022-04-12T21:15:42.085008Z","iopub.status.idle":"2022-04-12T21:15:42.125479Z","shell.execute_reply.started":"2022-04-12T21:15:42.084975Z","shell.execute_reply":"2022-04-12T21:15:42.123917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group all models per type of dataset\nmodel_counts = [popularity_count, cosine_count, pearson_count]\nmodels_dummy = [popularity_dummy, cosine_dummy, pearson_dummy]\n\nnames_counts = ['Popularity Model on Purchase Counts', 'Cosine Similarity on Purchase Counts',\n                'Pearson Similarity on Purchase Counts']\nnames_dummy = ['Popularity Model on Purchase Dummy', 'Cosine Similarity on Purchase Dummy',\n               'Pearson Similarity on Purchase Dummy']\n\n# # Evaluate the models\n# eval_counts = tc.recommender.util.compare_models(test_data, model_counts, model_names=names_counts, verbose=False)\n# eval_dummy = tc.recommender.util.compare_models(test_data_dummy, models_dummy, model_names=names_dummy, verbose=False)\n\n# üêù Save the updated `articles` file.\nsave_dataset_artifact(run_name=\"save_output_counts\", artifact_name=\"counts_results\",\n                      path=\"../input/hm-fashion-recommender-dataset/eval_counts.txt\")\n\nsave_dataset_artifact(run_name=\"save_output_dummy\", artifact_name=\"dummy_results\",\n                      path=\"../input/hm-fashion-recommender-dataset/eval_dummy.txt\")\n\n\ndel popularity_count, cosine_count, pearson_count\ndel popularity_dummy, cosine_dummy, pearson_dummy\ngc.collect()\n\n\nMETHOD = \"cosine\"\nTARGET = \"purchase_count\"\n\n# üêù W&B Experiment\nrun = wandb.init(project='HandM', name=f'{METHOD}_customers{TOP_CUSTOMERS}_topArticles{TOP_N}', config=CONFIG)\n\n\n# Final Model Training\nfinal_model = tc.item_similarity_recommender.create(tc.SFrame(train.to_pandas()), \n                                                    user_id=user_id, \n                                                    item_id=item_id, \n                                                    target=TARGET,\n                                                    similarity_type=METHOD,\n                                                    verbose=False)\n\nrecom = final_model.recommend(users=users_to_recommend, k=12, verbose=False)\n\n# Convert to dataframe\nrecom_df = recom.to_dataframe()\n\nprint(clr.S+\"12 Recommendations for each Customer:\"+clr.E)\nrecom_df.head(12)\n\n\n# Create predictions out of the dataset\nrecom_df = create_predictions_format(recom_df)\n\n\n# üêù Log CV score\nwandb.log({\"TOP_CUSTOMERS\" : TOP_CUSTOMERS,\n              \"TOP_N\" : TOP_N,\n              \"METHOD\" : METHOD,\n              \"TARGET\": TARGET,\n              \"CV\" : 0.0051})\n\nwandb.finish()\n\n\n# Import sample submission\nss = cudf.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv\")\nss['customer_id_new'] = ss['customer_id'].str[-16:].str.hex_to_int().astype('int64')\n\n\n# Merge with predicted preds\nss = ss.merge(cudf.DataFrame(recom_df[[\"customer_id\", \"preds\"]]), \n              left_on=\"customer_id_new\", right_on=\"customer_id\", how=\"left\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:12:50.961884Z","iopub.execute_input":"2022-04-12T21:12:50.964859Z","iopub.status.idle":"2022-04-12T21:12:51.005073Z","shell.execute_reply.started":"2022-04-12T21:12:50.962766Z","shell.execute_reply":"2022-04-12T21:12:51.003846Z"},"trusted":true},"execution_count":null,"outputs":[]}]}